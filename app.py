# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vRqMTcnPoOZDs7tb8Vhrsrxn7ek0KHmv
"""









import cv2
import torch
import numpy as np
import streamlit as st
from ultralytics import YOLO

# Load the MiDaS model for depth estimation
def load_midas_model():
    midas = torch.hub.load("intel-isl/MiDaS", "MiDaS_small")
    midas.to('cuda' if torch.cuda.is_available() else 'cpu')
    midas.eval()
    midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
    transform = midas_transforms.small_transform
    return midas, transform

# Capture a frame from the video
def capture_frame(video_capture):
    ret, frame = video_capture.read()
    return frame if ret else None

# YOLO model for object detection
def model_object_detection_predict(frame, yolo_model):
    results = yolo_model(frame)

    if len(results[0].boxes.data) == 0:
        print("No objects detected")
    else:
        print(f"Detected {len(results[0].boxes.data)} objects")

    return results

# Function to calculate the average depth for a bounding box
def calculate_average_depth(depth_map, bbox):
    x1, y1, x2, y2 = map(int, bbox)
    roi_depth = depth_map[y1:y2, x1:x2]
    return np.mean(roi_depth)

def model_depth_estimation_predict(frame, midas, transform, yolo_bboxes):
    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    input_batch = transform(img).to('cuda' if torch.cuda.is_available() else 'cpu')

    with torch.no_grad():
        prediction = midas(input_batch)
        prediction = torch.nn.functional.interpolate(
            prediction.unsqueeze(1),
            size=img.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze()

    depth_map = prediction.cpu().numpy()
    depth_map = cv2.normalize(depth_map, None, 0, 1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_64F)

    distances = []
    for bbox in yolo_bboxes:
        avg_depth = calculate_average_depth(depth_map, bbox)
        distances.append(avg_depth)

    return depth_map, distances

# Function to process video and display
def process_video(video_path):
    midas, transform = load_midas_model()

    # Load the YOLO model
    yolo_model = YOLO('C:\\Users\\rana2\\Downloads\\final_model.pt')  # Ensure correct model path

    video_capture = cv2.VideoCapture(video_path)
    if not video_capture.isOpened():
        st.error("Error opening video source")
        return

    stframe = st.empty()
    total_detections = 0  # Initialize total detections counter
    frame_count = 0  # Initialize frame count

    # Define class names
    class_names = ['bus', 'truck', 'car', 'motorcycle']

    # Define safe distance for warning
    safe_distance = 0.555  # You can adjust this value based on your use case
# 0.555
    while True:
        frame = capture_frame(video_capture)
        if frame is None:
            break

        frame_count += 1
        print(f"Processing frame {frame_count}")

        # Object detection
        od_results = model_object_detection_predict(frame, yolo_model)
        yolo_bboxes = [det[:4].tolist() for det in od_results[0].boxes.data]

        # Depth estimation
        depth_map, distances = model_depth_estimation_predict(frame, midas, transform, yolo_bboxes)

        total_detections += len(distances)  # Update total detections count

        # Create depth color map
        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_map, alpha=255), cv2.COLORMAP_JET)
        blended = cv2.addWeighted(frame, 0.6, depth_colormap, 0.4, 0)

        # Draw bounding boxes and labels on the frame
        for i, det in enumerate(od_results[0].boxes.data):
            bbox = det[:4].tolist()
            x1, y1, x2, y2 = map(int, bbox)
            avg_depth = distances[i]
            confidence = det[4]  # Confidence score from YOLO
            cls = int(det[5].item())  # Class index

            # Get class name
            vehicle_name = class_names[cls] if cls < len(class_names) else f"Unknown_{cls}"

            # Draw rectangle and add text
            cv2.rectangle(blended, (x1, y1), (x2, y2), (255, 0, 0), 2)
            label = f"{vehicle_name}, Conf: {confidence:.2f}, Depth: {avg_depth:.4f}"
            cv2.putText(blended, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)

            # Only display warning when vehicle is too close
            if avg_depth > safe_distance:
                warning = "WARNING: Too Close!"
                print(f"Object {i} is too close: {avg_depth} < {safe_distance}")  # Debugging message

                # Get warning text size
                (warn_width, warn_height), _ = cv2.getTextSize(warning, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)

                # Draw white background for warning
                cv2.rectangle(blended, (x1, y2), (x1 + warn_width, y2 + warn_height + 10), (255, 255, 255), -1)

                # Add red warning text
                cv2.putText(blended, warning, (x1, y2 + warn_height + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
            #else:
                print(f"Object {i} is safe: {avg_depth} >= {safe_distance}")  # Debugging message

        # Display the processed frame in Streamlit
        stframe.image(blended, channels="BGR")
        # Optional: break after processing a certain number of frames
        if frame_count >= 300:  # Process 10 seconds assuming 30 fps
            break

    video_capture.release()

    # Display total detections after processing
    #st.sidebar.success(f"Total detected objects: {total_detections}")

# Streamlit UI
st.set_page_config(page_title="ÿ±ÿßÿµŸêÿØ", layout="wide")
st.title("üöó ÿ±ÿßÿµŸêÿØ")

# Sidebar for user options
st.sidebar.header("Upload Video")
uploaded_file = st.sidebar.file_uploader("Choose a video file", type=["mp4", "mov"])

if uploaded_file is not None:
    # Save the uploaded file temporarily
    video_path = "temp_video.mp4"
    with open(video_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.sidebar.video(video_path)
    st.sidebar.success("Video uploaded successfully!")

    # Process the video with object detection and depth estimation
    process_video(video_path)
else:
    st.sidebar.info("Please upload a video to get started.")